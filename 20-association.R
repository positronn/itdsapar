# Association is not causation
library(tidyverse)
# There are many reasons that a variable X can be correlated with a variable Y without having any direct effect on Y . Here we examine three common ways that can lead to misinterpreting data.


# A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations
# among uncorrelated variables. We will save the results of our simulation into a tibble:
N <- 25
g <- 1000000
sim_data <- tibble(
    group = rep(1:g, each = N),
    x = rnorm(N * g),
    y = rnorm(N * g)
)

# The first column denotes group. We created groups and for each one we generated a pair of independent vectors, X and Y , with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that X and Y are not correlated.
# Next, we compute the correlation between X and Y for each group and look at the max:
res <- sim_data %>% 
    group_by(group) %>% 
    summarize(r = cor(x, y)) %>% 
    arrange(desc(r))
res

# We see a maximum correlation of 0.809 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that X and Y are in fact correlated:
sim_data %>% 
    filter(group == res$group[which.max(res$r)]) %>% 
    ggplot(aes(x, y)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = 'y ~ x')
# Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:
res %>% 
    ggplot(aes(x = r)) +
    geom_histogram(binwidth = 0.1, color = 'black')

# It’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.204, the largest one will be close 1.
# If we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:
library(broom)
sim_data %>% 
    filter(group == res$group[which.max(res$r)]) %>% 
    do(tidy(lm(y ~ x, data = .)))

# This particular form of data dredging is referred to as p-hacking. P-hacking is a
# topic of much discussion because it is a problem in scientific publications.
# Because publishers tend to reward statistically significant results over negative
# results, there is an incentive to report significant results. In epidemiology
# and the social sciences, for example, researchers may look for associations
# between an adverse outcome and several exposures and report only the one exposure
# that resulted in a small p-value. Furthermore, they might try fitting several
# different models to account for confounding and pick the one that yields the
# smallest p-value. In experimental disciplines, an experiment might be repeated
# more than once, yet only the results of the one experiment with a small
# p-value reported. This does not necessarily happen due to unethical behavior,
# but rather as a result of statistical ignorance or wishful thinking. In advanced
# statistics courses, you can learn methods to adjust for these multiple comparisons.


# 20.2 Outliers
# Suppose we take measurements from two independent outcomes, X and Y , and we standardize the measure- ments. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:
set.seed(1985)
x <- rnorm(100, 100, 1)
y <- rnorm(100, 84, 1)
x[-23] <- scale(x[-23])
y[-23] <- scale(y[-23])

# The data look like this:
ggplot() +
    geom_point(mapping = aes(x = x, y = y), alpha = 0.5)
# Not surprisingly, the correlation is very high:
cor(x, y)
# 
But#  this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:
    cor(x[-23], y[-23])

# There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:
    ggplot() +
        geom_point(mapping = aes(x = rank(x), y = rank(y)))

#  The outlier is no longer associated with a very large value and the correlation comes way down:
cor(rank(x), rank(y))


# Spearman correlation can also be calculated like this:
cor(x, y, method = 'kendal')


# 20.3 Reversing cause and effect
# Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.
library(HistData)
data('GaltonFamilies')
GaltonFamilies %>% 
    tibble() %>% 
    filter(childNum == 1 &
           gender == 'male') %>% 
    select(father, childHeight) %>% 
    rename(son = childHeight) %>% 
    reframe(tidy(lm(father ~ son, data = .)))

# The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.


# 20.4 Confounders
# Confounders are perhaps the most common reason that leads to associations begin misinterpreted.
# If X and Y are correlated, we call Z a confounder if changes in Z causes changes in both X and Y . Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.
# Incorrect interpretation due to confounders are ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.


# 20.4.1 Example: UC Berkeley admissions
# Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). Here is the data:
library(dslabs)
data(admissions)
admissions <- admissions %>% 
    tibble()
admissions

admissions %>% 
    group_by(gender) %>% 
    summarize(percentage = round(sum(admitted * applicants) / sum(applicants), 1))

# A statistical test clearly REJECTS the hypothesis that gender and admission are independent:
admissions %>% 
    group_by(gender) %>% 
    summarize(total_admitted = round(sum(admitted / 100 * applicants)),
              not_admitted = sum(applicants) - sum(total_admitted)) %>% 
    select(-gender) %>% 
    reframe(tidy(chisq.test(.)))

# But closer inspection shows a paradoxical result. Here are the percent admissions by major:
admissions %>% 
    select(major, gender, admitted) %>% 
    spread(gender, admitted) %>% 
    mutate(women_minus_men = women - men)

# Four out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.
# The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.
# So let’s define three variables: X is 1 for men and 0 for women, Y is 1 for those admitted and 0 otherwise, and Z quantifies the selectivity of the major. A gender bias claim would be based on the fact that Pr(Y = 1|X = x) is higher for x = 1 than x = 0. However, Z is an important confounder to consider. Clearly Z is associated with Y , as the more selective a major, the lower Pr(Y = 1|Z = z). But is major selectivity Z associated with gender X?
# One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:
admissions %>% 
    group_by(major) %>% 
    summarize(major_selectivity = sum(admitted) / sum(applicants),
              percent_women_applicants = sum(applicants * (gender == 'women')) /
                                         sum(applicants) * 100) %>% 
    ggplot(mapping = aes(x = major_selectivity, y = percent_women_applicants, label = major)) +
    geom_text()

# There seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while
# less than 30% of the applicants of major B were women.
admissions %>% 
    mutate(group = paste0(major,'-', gender)) %>% 
    ggplot() +
    geom_col(mapping = aes(y = group, x = applicants, group = major, fill = gender),
             position = 'dodge', alpha = 0.7) +
    geom_col(mapping = aes(y = group, x = admitted, group = major, fill = gender),
             position = 'dodge', alpha = 0.7) +
    geom_text(aes(y = group, x = applicants, label = admitted)) +
    theme(legend.position = 'none')


# The following plot shows the number of applicants that were admitted and those that were not by:
admissions %>% 
    mutate(yes = round(admitted / 100 * applicants),
           no = applicants - yes) %>% 
    select(-applicants, -admitted) %>% 
    gather(admission, number_of_students, -c('major', 'gender')) %>% 
    ggplot(aes(gender, number_of_students, fill = admission)) +
    geom_bar(stat = 'identity', position = 'stack') +
    facet_wrap(. ~ major)

admissions %>% 
    mutate(percent_admitted = admitted * applicants / sum(applicants)) %>% 
    ggplot(aes(gender, y = percent_admitted, fill = major)) +
    geom_bar(stat = 'identity', position = 'stack')

# It also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.


# 20.4.3 Average after stratifying
# In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:
admissions %>% 
    ggplot(aes(major, admitted, col = gender, size = applicants)) +
    geom_point()

# Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.
# If we average the difference by major, we find that the percent is actually 3.5% higher for women.
admissions %>% 
    group_by(gender) %>% 
    summarize(average = mean(admitted))


# 20.5 Simpson’s paradox
